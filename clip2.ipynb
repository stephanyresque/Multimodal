{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Clip"
      ],
      "metadata": {
        "id": "fo-XHdYQwxSK"
      },
      "id": "fo-XHdYQwxSK"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Instalações de pacotes"
      ],
      "metadata": {
        "id": "z0WoxpFrw0Qn"
      },
      "id": "z0WoxpFrw0Qn"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a2829ec",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "6a2829ec"
      },
      "outputs": [],
      "source": [
        "# !pip install torch torchvision transformers datasets tqdm\n",
        "# !pip install datasets[image] pillow"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports iniciais"
      ],
      "metadata": {
        "id": "8F8RZcSOxNoN"
      },
      "id": "8F8RZcSOxNoN"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c555a4a2",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "c555a4a2"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from zipfile import ZipFile, BadZipFile\n",
        "import random\n",
        "import pickle\n",
        "import heapq\n",
        "import json\n",
        "from collections import defaultdict\n",
        "\n",
        "import torch\n",
        "from transformers import CLIPModel, CLIPProcessor\n",
        "from PIL import Image\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset COCO"
      ],
      "metadata": {
        "id": "2dALBFEoxROw"
      },
      "id": "2dALBFEoxROw"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86c37e57",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "86c37e57"
      },
      "outputs": [],
      "source": [
        "!wget http://images.cocodataset.org/zips/train2017.zip -O coco_train2017.zip\n",
        "!wget http://images.cocodataset.org/zips/val2017.zip -O coco_val2017.zip\n",
        "!wget http://images.cocodataset.org/annotations/annotations_trainval2017.zip -O coco_ann2017.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7add931",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "f7add931"
      },
      "outputs": [],
      "source": [
        "# 2. Método para extrair arquivos ZIP\n",
        "def extrair_arquivo(caminho):\n",
        "    try:\n",
        "        with ZipFile(caminho + \".zip\") as zipf:\n",
        "            zipf.extractall(caminho)\n",
        "        os.remove(caminho + \".zip\")\n",
        "    except BadZipFile as e:\n",
        "        print(\"Erro ao extrair:\", e)\n",
        "\n",
        "# Extrair dados do COCO\n",
        "extrair_arquivo(\"./coco_train2017\")\n",
        "extrair_arquivo(\"./coco_val2017\")\n",
        "extrair_arquivo(\"./coco_ann2017\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Carregando modelo CLIP"
      ],
      "metadata": {
        "id": "7XTX996KxTyC"
      },
      "id": "7XTX996KxTyC"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1d6ee0d",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "d1d6ee0d"
      },
      "outputs": [],
      "source": [
        "# 3. Configurações iniciais\n",
        "dispositivo = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "diretorio_val = \"./coco_val2017/val2017\"\n",
        "caminho_anotacoes = \"./coco_ann2017/annotations/captions_val2017.json\"\n",
        "\n",
        "# 4. Carregar modelo CLIP\n",
        "modelo = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(dispositivo)\n",
        "processador = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "modelo.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Organização dos dados"
      ],
      "metadata": {
        "id": "OA2ZuXqZxZoL"
      },
      "id": "OA2ZuXqZxZoL"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3cf49af",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "f3cf49af"
      },
      "outputs": [],
      "source": [
        "# Precomputar embeddings das imagens de validação\n",
        "caminhos_imagens = [\n",
        "    os.path.join(diretorio_val, f)\n",
        "    for f in os.listdir(diretorio_val) if f.endswith(\".jpg\")\n",
        "]\n",
        "embeddings_imagem = {}\n",
        "for caminho in tqdm(caminhos_imagens, desc=\"Pré-computando embeddings\"):\n",
        "    img = Image.open(caminho).convert(\"RGB\")\n",
        "    inputs = processador(images=img, return_tensors=\"pt\").to(dispositivo)\n",
        "    with torch.no_grad():\n",
        "        emb = modelo.get_image_features(**inputs)\n",
        "        emb = emb / emb.norm(dim=-1, keepdim=True)\n",
        "    img_id = int(os.path.basename(caminho).split(\".\")[0])\n",
        "    embeddings_imagem[img_id] = emb.cpu()\n",
        "\n",
        "# Salvar embeddings em arquivo para reutilização\n",
        "with open(\"embeddings_val.pkl\", \"wb\") as f:\n",
        "    pickle.dump(embeddings_imagem, f)\n",
        "\n",
        "# Carregar legendas e organizar por ID de imagem\n",
        "with open(caminho_anotacoes, 'r') as f:\n",
        "    anotacoes = json.load(f)\n",
        "legendas_por_imagem = defaultdict(list)\n",
        "for entry in anotacoes['annotations']:\n",
        "    img_id = entry['image_id']\n",
        "    legendas_por_imagem[img_id].append(entry['caption'])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Funções auxiliares"
      ],
      "metadata": {
        "id": "QLU9chXsxd3-"
      },
      "id": "QLU9chXsxd3-"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b891442d",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "b891442d"
      },
      "outputs": [],
      "source": [
        "# Função para criar exemplos com mistura de distractores difíceis e fáceis\n",
        "def criar_exemplo_calibrado(\n",
        "    id_alvo, embeddings, modelo, processador,\n",
        "    k=9, tamanho_pool=2000, n_facil=7\n",
        "):\n",
        "    # Selecionar pool de imagens\n",
        "    pool = random.sample(list(embeddings.keys()), tamanho_pool)\n",
        "    pool = [i for i in pool if i != id_alvo]\n",
        "    # Escolher legenda\n",
        "    legenda = random.choice(legendas_por_imagem[id_alvo])\n",
        "    # Computar embedding de texto\n",
        "    inputs = processador(text=[legenda], return_tensors=\"pt\", padding=True).to(dispositivo)\n",
        "    with torch.no_grad():\n",
        "        emb_texto = modelo.get_text_features(**inputs)\n",
        "        emb_texto = emb_texto / emb_texto.norm(dim=-1, keepdim=True)\n",
        "        emb_texto = emb_texto.cpu()\n",
        "    # Calcular similaridade com imagens no pool\n",
        "    sims = [(float((emb_texto @ embeddings[i].T)), i) for i in pool]\n",
        "    # Selecionar distractores difíceis\n",
        "    top_duros = [i for _, i in heapq.nlargest(k - n_facil, sims, key=lambda x: x[0])]\n",
        "    # Adicionar distractores fáceis\n",
        "    faciles = random.sample([i for i in pool if i not in top_duros], n_facil)\n",
        "    distractores = top_duros + faciles\n",
        "    return {\n",
        "        \"legenda\": legenda,\n",
        "        \"id_correto\": id_alvo,\n",
        "        \"ids_distratores\": distractores\n",
        "    }\n",
        "\n",
        "# Função para calcular Recall@1\n",
        "def calcular_recall_1(exemplo, modelo, processador):\n",
        "    # Preparar imagens\n",
        "    ids = [exemplo[\"id_correto\"]] + exemplo[\"ids_distratores\"]\n",
        "    imagens = [\n",
        "        Image.open(f\"{diretorio_val}/{str(i).zfill(12)}.jpg\") for i in ids\n",
        "    ]\n",
        "    # Processar entrada CLIP\n",
        "    inputs = processador(text=[exemplo[\"legenda\"]], images=imagens,\n",
        "                         return_tensors=\"pt\", padding=True).to(dispositivo)\n",
        "    with torch.no_grad():\n",
        "        saida = modelo(**inputs)\n",
        "        logits = saida.logits_per_text.cpu().numpy()[0]\n",
        "    # Verificar top-1\n",
        "    indice_top1 = logits.argmax()\n",
        "    return int(ids[indice_top1] == exemplo[\"id_correto\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Calculando recall 1"
      ],
      "metadata": {
        "id": "-3hz8rqYxi7q"
      },
      "id": "-3hz8rqYxi7q"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90ad7ed6",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "90ad7ed6"
      },
      "outputs": [],
      "source": [
        "# Carregar embeddings do arquivo\n",
        "with open(\"embeddings_val.pkl\", \"rb\") as f:\n",
        "    embeddings_imagem = pickle.load(f)\n",
        "\n",
        "# Gerar exemplos calibrados e avaliar\n",
        "N = 2000\n",
        "exemplos = [\n",
        "    criar_exemplo_calibrado(\n",
        "        id_alvo=random.choice(list(legendas_por_imagem.keys())),\n",
        "        embeddings=embeddings_imagem,\n",
        "        modelo=modelo,\n",
        "        processador=processador,\n",
        "        k=9,\n",
        "        tamanho_pool=2000,\n",
        "        n_facil=7\n",
        "    )\n",
        "    for _ in range(N)\n",
        "]\n",
        "\n",
        "# Calcular recall geral\n",
        "acertos = sum(calcular_recall_1(ex, modelo, processador) for ex in tqdm(exemplos))\n",
        "print(f\"Recall@1 calibrado: {acertos / N:.2%}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Analisando os erros"
      ],
      "metadata": {
        "id": "l2SJBsggxnVI"
      },
      "id": "l2SJBsggxnVI"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "116fd290",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "116fd290"
      },
      "outputs": [],
      "source": [
        "import random, numpy as np, torch\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available(): torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "\n",
        "def calcular_recall_k(exemplo, modelo, processador, k=5):\n",
        "    ids = [exemplo[\"id_correto\"]] + exemplo[\"ids_distratores\"]\n",
        "    imagens = [Image.open(f\"{diretorio_val}/{str(i).zfill(12)}.jpg\") for i in ids]\n",
        "    inputs = processador(text=[exemplo[\"legenda\"]], images=imagens,\n",
        "                         return_tensors=\"pt\", padding=True).to(dispositivo)\n",
        "    with torch.no_grad():\n",
        "        logits = modelo(**inputs).logits_per_text.cpu().numpy()[0]\n",
        "    topk = logits.argsort()[-k:][::-1]\n",
        "    return int(exemplo[\"id_correto\"] in [ids[idx] for idx in topk])\n",
        "\n",
        "def calcular_mrr(exemplo, modelo, processador):\n",
        "    ids = [exemplo[\"id_correto\"]] + exemplo[\"ids_distratores\"]\n",
        "    imagens = [Image.open(f\"{diretorio_val}/{str(i).zfill(12)}.jpg\") for i in ids]\n",
        "    inputs = processador(text=[exemplo[\"legenda\"]], images=imagens,\n",
        "                         return_tensors=\"pt\", padding=True).to(dispositivo)\n",
        "    with torch.no_grad():\n",
        "        logits = modelo(**inputs).logits_per_text.cpu().numpy()[0]\n",
        "    ranks = logits.argsort()[::-1]\n",
        "    rank_pos = list(ranks).index(0) + 1\n",
        "    return 1.0 / rank_pos\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "regs = []\n",
        "for ex in tqdm(exemplos, desc=\"Rodando avaliação detalhada\"):\n",
        "    regs.append({\n",
        "        \"legenda\": ex[\"legenda\"],\n",
        "        \"hit@1\": calcular_recall_1(ex, modelo, processador),\n",
        "        \"hit@5\": calcular_recall_k(ex, modelo, processador, k=5),\n",
        "        \"mrr\":   calcular_mrr(ex, modelo, processador)\n",
        "    })\n",
        "df = pd.DataFrame(regs)\n",
        "print(\"Recall@1:\", df[\"hit@1\"].mean(),\n",
        "      \"Recall@5:\", df[\"hit@5\"].mean(),\n",
        "      \"MRR:\", df[\"mrr\"].mean())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "161f3963",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "161f3963"
      },
      "outputs": [],
      "source": [
        "df[\"num_tokens\"]    = df[\"legenda\"].apply(lambda t: len(t.split()))\n",
        "df[\"tamanho_chars\"] = df[\"legenda\"].str.len()\n",
        "\n",
        "# Função de classificação de erro\n",
        "import re\n",
        "\n",
        "def classificar_erro(legenda):\n",
        "    if len(legenda) < 3:\n",
        "        return \"muito_curta\"\n",
        "    if len(legenda) > 50:\n",
        "        return \"muito_longa\"\n",
        "    if re.search(r\"\\b(e|and|,)\\b\", legenda):\n",
        "        return \"ambigua\"\n",
        "    return \"outros\"\n",
        "\n",
        "df_fail = df[df[\"hit@1\"] == 0].copy()\n",
        "df_fail[\"tipo_erro\"] = df_fail[\"legenda\"].apply(classificar_erro)\n",
        "\n",
        "\n",
        "# Análise de distribuições\n",
        "print(\"Distribuição de erros:\")\n",
        "print(df_fail[\"tipo_erro\"].value_counts(normalize=True))\n",
        "\n",
        "# visualizar contagens absolutas também\n",
        "print(df_fail[\"tipo_erro\"].value_counts())\n",
        "\n",
        "\n",
        "# Explorar exemplos de cada classe\n",
        "for categoria in df_fail[\"tipo_erro\"].unique():\n",
        "    print(f\"\\n=== Exemplos de {categoria} ===\")\n",
        "    display(df_fail[df_fail[\"tipo_erro\"] == categoria].head(5)[[\"legenda\"]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eedf16d9",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "eedf16d9"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "vc = df_fail[\"tipo_erro\"].value_counts(normalize=True)\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "vc.plot.bar()\n",
        "plt.title(\"Distribuição de tipos de erro (Recall@1 falhos)\")\n",
        "plt.ylabel(\"Proporção\")\n",
        "plt.xlabel(\"Tipo de erro\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "top3 = vc.head(3)\n",
        "print(\"Top-3 categorias de erro:\")\n",
        "for cat, pct in top3.items():\n",
        "    print(f\"  • {cat}: {pct:.1%}\")\n",
        "\n",
        "for cat in top3.index:\n",
        "    print(f\"\\n=== Exemplos de {cat} ===\")\n",
        "    display(df_fail[df_fail[\"tipo_erro\"] == cat].head(3)[[\"legenda\"]])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d00d8720",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "d00d8720"
      },
      "outputs": [],
      "source": [
        "#Filtrar só os erros “outros”\n",
        "df_outros = df_fail[df_fail[\"tipo_erro\"] == \"outros\"].copy()\n",
        "\n",
        "print(\"Estatísticas de palavras (outros):\")\n",
        "print(df_outros[\"num_tokens\"].describe(), \"\\n\")\n",
        "print(\"Estatísticas de caracteres (outros):\")\n",
        "print(df_outros[\"tamanho_chars\"].describe(), \"\\n\")\n",
        "\n",
        "print(\"===== 10 exemplos aleatórios de 'outros' =====\")\n",
        "for leg in df_outros[\"legenda\"].sample(10, random_state=42).tolist():\n",
        "    print(\" •\", leg)\n",
        "print(\"\\n\")\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "vectorizer = CountVectorizer(\n",
        "    ngram_range=(1,2),\n",
        "    max_features=20,\n",
        "    stop_words=None\n",
        ")\n",
        "X = vectorizer.fit_transform(df_outros[\"legenda\"])\n",
        "freqs = X.toarray().sum(axis=0)\n",
        "ngrams = vectorizer.get_feature_names_out()\n",
        "\n",
        "print(\"Top 20 uni-/bi-grams mais comuns em 'outros':\")\n",
        "for ng, f in sorted(zip(ngrams, freqs), key=lambda x: x[1], reverse=True):\n",
        "    print(f\"{ng}: {f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "07934bb9",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "07934bb9"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "errs = []\n",
        "for ex, hit in zip(exemplos, df[\"hit@1\"]):\n",
        "    if hit == 0:\n",
        "        errs.append({\"id\": ex[\"id_correto\"], \"legenda\": ex[\"legenda\"]})\n",
        "\n",
        "N = 9\n",
        "sample = random.sample(errs, N)\n",
        "\n",
        "fig, axes = plt.subplots(3, 3, figsize=(12,12))\n",
        "for ax, item in zip(axes.flatten(), sample):\n",
        "    img_id = item[\"id\"]\n",
        "    caption = item[\"legenda\"]\n",
        "    path = f\"{diretorio_val}/{str(img_id).zfill(12)}.jpg\"\n",
        "    img = Image.open(path)\n",
        "    ax.imshow(img)\n",
        "    ax.set_title(caption, fontsize=10)\n",
        "    ax.axis(\"off\")\n",
        "\n",
        "plt.suptitle(\"Exemplos de imagens mal recuperadas (Recall@1 até 10 opções)\", fontsize=14)\n",
        "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52c5bc2e",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "52c5bc2e"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import pandas as pd\n",
        "\n",
        "N = 50\n",
        "\n",
        "legendas_amb = df_fail[df_fail[\"tipo_erro\"] == \"ambigua\"][\"legenda\"] \\\n",
        "                  .sample(N, random_state=42) \\\n",
        "                  .reset_index(drop=True)\n",
        "\n",
        "legendas_out = df_fail[df_fail[\"tipo_erro\"] == \"outros\"][\"legenda\"] \\\n",
        "                  .sample(N, random_state=42) \\\n",
        "                  .reset_index(drop=True)\n",
        "\n",
        "df_compare = pd.DataFrame({\n",
        "    \"Ambíguas\": legendas_amb,\n",
        "    \"Outros\":   legendas_out\n",
        "})\n",
        "\n",
        "display(df_compare)\n",
        "\n",
        "for i in range(N):\n",
        "    print(f\"{i+1:2d}. A: {df_compare.loc[i, 'Ambíguas']}\")\n",
        "    print(f\"    B: {df_compare.loc[i, 'Outros']}\\n\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}